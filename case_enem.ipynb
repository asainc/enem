{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCB04IdznXXq"
   },
   "source": [
    "# ENEM 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIxxcVy5nb85"
   },
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRaFhSQp0e80"
   },
   "source": [
    "The ENEM is the largest educational exam in Brazil, and its microdata base contains millions of records that represent a snapshot of Brazilian education. However:\n",
    "\n",
    "â— **Problem (Pain Point)**\n",
    "\n",
    "Today, schools, education networks, and analysts have great difficulty in anticipating student performance before the exam is administered. This leads to:\n",
    "\n",
    "- Reactive pedagogical planning (corrections after the results, not before).\n",
    "- Difficulty in customizing study paths.\n",
    "- Impossibility of simulating scenarios (e.g., impact of attendance, type of school, socioeconomic profiles).\n",
    "- Low predictability of school performance, harming public policies.\n",
    "\n",
    "ðŸ’¡ **Opportunity**\n",
    "\n",
    "Build a predictive model capable of estimating the four objective scores of the ENEM (excluding the essay) based on:\n",
    "\n",
    "- student profile,\n",
    "- socioeconomic conditions,\n",
    "- school information,\n",
    "- exam status (attendance, language, etc.).\n",
    "\n",
    "With this, schools and administrators can:\n",
    "\n",
    "- Create personalized pedagogical plans;\n",
    "- Anticipate at-risk groups with low performance;\n",
    "- Simulate the impact of educational policies;\n",
    "- Increase resource alignment.\n",
    "\n",
    "âœ… **Solution**\n",
    "\n",
    "Create a model that receives a single set of features and generates four predicted scores simultaneously:\n",
    "\n",
    "- NU_NOTA_CN\n",
    "- NU_NOTA_CH\n",
    "- NU_NOTA_LC\n",
    "- NU_NOTA_MT\n",
    "\n",
    "**Candidate models:**\n",
    "- Lasso Multioutput\n",
    "- Random Forest Multioutput\n",
    "- XGBoost Multioutput\n",
    "\n",
    "ðŸŸ© **Advantages**\n",
    "\n",
    "- Integration between the four competencies (the scores are correlated).\n",
    "- Unique and reduced architecture.\n",
    "- More consistent and easier to explain model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn1Ga4isnemh"
   },
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQbEm80L88iM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGvq-KutoWER",
    "outputId": "f9bbddc2-1b44-4c31-a05d-d90798046d8b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kstest, shapiro, norm\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import sqlite3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import shap\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "from uvicorn.config import Config\n",
    "from uvicorn.server import Server\n",
    "from fastapi import FastAPI, Depends, HTTPException, status\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from pydantic import BaseModel\n",
    "import pickle\n",
    "import nest_asyncio\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kchFWm8aoYog"
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('database.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD6coXx9oWbb"
   },
   "outputs": [],
   "source": [
    "# Filtering only people who have taken all four tests and who are not practice test takers.\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM enem\n",
    "WHERE\n",
    "(TP_PRESENCA_CN = 1)\n",
    "AND (TP_PRESENCA_CH = 1)\n",
    "AND (TP_PRESENCA_LC = 1)\n",
    "AND (TP_PRESENCA_MT = 1)\n",
    "AND (NU_NOTA_CN > 0 and NU_NOTA_CN is not null)\n",
    "AND (NU_NOTA_CH > 0 and NU_NOTA_CH is not null)\n",
    "AND (NU_NOTA_LC > 0 and NU_NOTA_LC is not null)\n",
    "AND (NU_NOTA_MT > 0 and NU_NOTA_MT is not null)\n",
    "AND (IN_TREINEIRO == 0)\n",
    "\"\"\"\n",
    "\n",
    "df_raw = pd.read_sql_query(query, conn)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQo3gmUqI_-v"
   },
   "outputs": [],
   "source": [
    "def multi_stratified_sample(\n",
    "    df: pd.DataFrame,\n",
    "    strata_cols: List[str],\n",
    "    n_samples: int,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs stratified sampling in multiple columns.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with millions of rows.\n",
    "    strata_cols : list\n",
    "        List of columns which defines the strata.\n",
    "    n_samples : int\n",
    "        Number of desired samples.\n",
    "    random_state : int\n",
    "        Seed for reproducibility.\n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    pd.DataFrame : stratified sample.\n",
    "    \"\"\"\n",
    "    # Ensure reproducibility\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Simple Verifcation\n",
    "    if n_samples > len(df):\n",
    "        raise ValueError(\"n_samples cannot be greater than the size of the DataFrame.\")\n",
    "\n",
    "    # Create groups by strata\n",
    "    grouped = df.groupby(strata_cols)\n",
    "\n",
    "    # Size of each group (stratum)\n",
    "    group_sizes = grouped.size()\n",
    "\n",
    "    # Proportion of each stratum\n",
    "    proportions = group_sizes / group_sizes.sum()\n",
    "\n",
    "    # Number of samples per stratum (proportional)\n",
    "    samples_per_group = (proportions * n_samples).round().astype(int)\n",
    "\n",
    "    # Ensure that the final sum is exactly n_samples\n",
    "    diff = n_samples - samples_per_group.sum()\n",
    "\n",
    "    if diff != 0:\n",
    "        # Adjust the largest strata until they exactly match n_samples\n",
    "        largest_groups = samples_per_group.sort_values(ascending=False)\n",
    "        samples_per_group.loc[largest_groups.index[:abs(diff)]] += np.sign(diff)\n",
    "\n",
    "    # Perform stratified sampling\n",
    "    sampled_df = (\n",
    "        grouped\n",
    "        .apply(lambda x: x.sample(n=samples_per_group.loc[x.name], random_state=random_state, replace=False))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fioRRAXFJAKG"
   },
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "df = multi_stratified_sample(df, ['SG_UF_ESC','TP_ST_CONCLUSAO','TP_ESCOLA','TP_ENSINO','TP_COR_RACA','TP_SEXO'], 30000, random_state=42)\n",
    "\n",
    "relevant_columns = ['TP_FAIXA_ETARIA', 'TP_SEXO','TP_ESTADO_CIVIL', 'TP_COR_RACA','TP_ST_CONCLUSAO',\n",
    "                    'TP_ESCOLA', 'TP_ENSINO','SG_UF_ESC','TP_LOCALIZACAO_ESC', 'NU_NOTA_CN',\n",
    "                    'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO', 'Q001', 'Q002', 'Q003', 'Q004',\n",
    "                    'Q005', 'Q006', 'Q007', 'Q008', 'Q009', 'Q010', 'Q011', 'Q012', 'Q013',\n",
    "                    'Q014', 'Q015', 'Q016', 'Q017', 'Q018', 'Q019', 'Q020', 'Q021', 'Q022',\n",
    "                    'Q023', 'Q024', 'Q025','Q027']\n",
    "\n",
    "df = df[relevant_columns]\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqeQ-7OIJAVF"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFUbxFAsLGyT"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0UV5jsELEuq"
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh_-7jGyMDzm"
   },
   "source": [
    "## Checking for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8Md3y55KT85"
   },
   "outputs": [],
   "source": [
    "print(\"Missing values in the dataset:\",df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaIyju4qMIR1"
   },
   "source": [
    "## Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZ_UJnDRKUWT"
   },
   "outputs": [],
   "source": [
    "# Check if there are duplicates\n",
    "duplicates = df.duplicated()\n",
    "\n",
    "if duplicates.any():\n",
    "    # Option 1: Print the entire row with duplicates\n",
    "    print(df[duplicates])\n",
    "\n",
    "    # Option 2: Count the number of duplicates\n",
    "    print(duplicates.sum(), \"duplicate rows found\")\n",
    "else:\n",
    "    print(\"No duplicate rows found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzQMuG7RKUZs"
   },
   "outputs": [],
   "source": [
    "def detect_outliers_multi_boxplot(\n",
    "    df: pd.DataFrame,\n",
    "    columns: List[str],\n",
    "    figsize: Tuple[int, int] = (16, 6),\n",
    "    title_size: int = 18\n",
    ") -> Figure:\n",
    "    \"\"\"\n",
    "    Identifies outliers for multiple numeric columns using the IQR method\n",
    "    and plots side-by-side boxplots with a polished executive-style layout.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataset\n",
    "    columns : list\n",
    "        List of numeric columns to analyze\n",
    "    figsize : tuple\n",
    "        Size of the figure\n",
    "    title_size : int\n",
    "        Title font size\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Summary table with quartiles, bounds, outlier count and percentage\n",
    "    \"\"\"\n",
    "\n",
    "    stats_list = []\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Colors\n",
    "    box_color = \"#4C72B0\"\n",
    "    outlier_color = \"#DD8452\"\n",
    "\n",
    "    # Create subplots\n",
    "    num_cols = len(columns)\n",
    "    fig, axes = plt.subplots(1, num_cols, figsize=figsize)\n",
    "\n",
    "    # Ensure axes is iterable even if 1 column\n",
    "    if num_cols == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "\n",
    "        series = df[col].dropna()\n",
    "\n",
    "        # Quartiles & IQR\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "\n",
    "        # Store stats\n",
    "        stats_list.append({\n",
    "            \"column\": col,\n",
    "            \"Q1\": Q1,\n",
    "            \"Q3\": Q3,\n",
    "            \"IQR\": IQR,\n",
    "            \"lower_bound\": lower_bound,\n",
    "            \"upper_bound\": upper_bound,\n",
    "            \"outlier_count\": len(outliers),\n",
    "            \"outlier_percentage\": round(100 * len(outliers) / len(series), 2)\n",
    "        })\n",
    "\n",
    "        # Plot boxplot\n",
    "        sns.boxplot(\n",
    "            y=series,\n",
    "            ax=axes[i],\n",
    "            color=box_color,\n",
    "            width=0.4,\n",
    "            fliersize=7,\n",
    "            flierprops=dict(\n",
    "                marker='o',\n",
    "                markerfacecolor=outlier_color,\n",
    "                markersize=8,\n",
    "                linestyle='none'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        axes[i].set_title(f\"{col}\", fontsize=title_size, fontweight=\"bold\")\n",
    "        axes[i].grid(axis='y', linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "        # Add lines for bounds\n",
    "        axes[i].axhline(lower_bound, color=\"#C44E52\", linestyle=\"--\", linewidth=1.5)\n",
    "        axes[i].axhline(upper_bound, color=\"#55A868\", linestyle=\"--\", linewidth=1.5)\n",
    "\n",
    "    plt.suptitle(\"Outlier Analysis\", fontsize=title_size+2, fontweight=\"bold\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # Convert statistics to DataFrame\n",
    "    summary_df = pd.DataFrame(stats_list)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n===== Outlier Analysis Summary =====\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujUF4HHqM1gq"
   },
   "outputs": [],
   "source": [
    "detect_outliers_multi_boxplot(df, ['NU_NOTA_CN','NU_NOTA_CH','NU_NOTA_LC','NU_NOTA_MT'], figsize=(10, 5), title_size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5E7tpdtMPaX"
   },
   "outputs": [],
   "source": [
    "# Creating a copy of original dataset to change name variables for readability\n",
    "df_EDA = df.copy()\n",
    "\n",
    "# Creating 6 subplots\n",
    "fig, ax = plt.subplots(2, 3, figsize=(19, 18))\n",
    "\n",
    "# Age range\n",
    "df_EDA = df_EDA.sort_values(by='TP_FAIXA_ETARIA')\n",
    "df_EDA['TP_FAIXA_ETARIA'] = df_EDA['TP_FAIXA_ETARIA'].map({\n",
    "    1:'<17',\n",
    "    2:'17',\n",
    "    3:'18',\n",
    "    4:'19',\n",
    "    5:'20',\n",
    "    6:'21',\n",
    "    7:'22',\n",
    "    8:'23',\n",
    "    9:'24',\n",
    "    10:'25',\n",
    "    11:'26-30',\n",
    "    12:'31-35',\n",
    "    13:'36-40',\n",
    "    14:'41-45',\n",
    "    15:'46-50',\n",
    "    16:'51-55',\n",
    "    17:'56-60',\n",
    "    18:'61-65'\n",
    "})\n",
    "\n",
    "ax1 = ax[0,0]\n",
    "sns.countplot(x=df_EDA['TP_FAIXA_ETARIA'], ax=ax1)\n",
    "\n",
    "ax1.set(xlabel='Age range (years)',\n",
    "        ylabel='Number of candidates',\n",
    "        title='TP_FAIXA_ETARIA: Age range')\n",
    "\n",
    "for label in ax1.get_xticklabels():\n",
    "    label.set_rotation(60)\n",
    "ax1.grid(False)\n",
    "\n",
    "# Sex\n",
    "df_EDA['TP_SEXO'] = df_EDA['TP_SEXO'].map({'F':'Female','M':'Male'})\n",
    "ax2 = ax[0,1]\n",
    "sns.countplot(x=df_EDA['TP_SEXO'], ax=ax2)\n",
    "\n",
    "ax2.set(xlabel='Sex', ylabel='Number of candidates',title='TP_SEXO: Sex')\n",
    "ax2.grid(False)\n",
    "\n",
    "# Ethnicity\n",
    "df_EDA['TP_COR_RACA'] = df_EDA['TP_COR_RACA'].map({\n",
    "    0:'Not declared',\n",
    "    1:'White',\n",
    "    2:'Black',\n",
    "    3:'Pardo',\n",
    "    4:'Yellow',\n",
    "    5:'Indiginous'\n",
    "})\n",
    "ax3 = ax[0,2]\n",
    "sns.countplot(x=df_EDA['TP_COR_RACA'], ax=ax3, order=df_EDA['TP_COR_RACA'].value_counts(ascending=False).index)\n",
    "\n",
    "ax3.set(xlabel='Ethnicity', ylabel='Number of candidates',title='TP_COR_RACA: Ethnicity')\n",
    "for label in ax3.get_xticklabels():\n",
    "    label.set_rotation(20)\n",
    "ax3.grid(False)\n",
    "\n",
    "\n",
    "# School (Private, Public)\n",
    "df_EDA['TP_ESCOLA'] = df_EDA['TP_ESCOLA'].map({1:'Not declared',2:'Public',3:'Exterior',4:'Private'})\n",
    "ax4 = ax[1,0]\n",
    "sns.countplot(x=df_EDA['TP_ESCOLA'], ax=ax4, order=df_EDA['TP_ESCOLA'].value_counts(ascending=False).index)\n",
    "\n",
    "ax4.set(xlabel='Type of school', ylabel='Number of candidates',title='TP_ESCOLA: Type of School')\n",
    "for label in ax4.get_xticklabels():\n",
    "    label.set_rotation(20)\n",
    "ax4.grid(False)\n",
    "\n",
    "# Situation high school (finished, finishing)\n",
    "df_EDA['TP_ST_CONCLUSAO'] = df_EDA['TP_ST_CONCLUSAO'].map({\n",
    "    1:'Yes',\n",
    "    2:'No, will in 2018',\n",
    "    3:'No, will after 2018',\n",
    "    4:'No and will not'\n",
    "    })\n",
    "ax5 = ax[1,1]\n",
    "sns.countplot(x=df_EDA['TP_ST_CONCLUSAO'], ax=ax5, order=df_EDA['TP_ST_CONCLUSAO'].value_counts(ascending=False).index)\n",
    "\n",
    "ax5.set(xlabel='Finished high school?', ylabel='Number of candidates',title='TP_ST_CONCLUSAO: High School Situation')\n",
    "for label in ax5.get_xticklabels():\n",
    "    label.set_rotation(20)\n",
    "ax5.grid(False)\n",
    "\n",
    "# Family income\n",
    "df_EDA = df_EDA.sort_values(by='Q006')\n",
    "ax6 = ax[1,2]\n",
    "sns.countplot(x=df_EDA['Q006'], ax=ax6)\n",
    "ax6.set(xlabel='Family income class (A: 0 to Q: > R$19K)', ylabel='Number of candidates',title='Q006: Family Income')\n",
    "for label in ax6.get_xticklabels():\n",
    "    label.set_rotation(90)\n",
    "ax6.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqC7u2riQjY9"
   },
   "outputs": [],
   "source": [
    "# Creating subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Natural Sciences\n",
    "ax1 = ax[0]\n",
    "sns.histplot(x=df_EDA['NU_NOTA_CN'], bins=int(180/5), color='blue', edgecolor='black', ax=ax1, alpha=0.2)\n",
    "mean_cn = df_EDA['NU_NOTA_CN'].mean()\n",
    "median_cn = df_EDA['NU_NOTA_CN'].median()\n",
    "ax1.axvline(x=mean_cn, color='red', linestyle='--', label='Mean')\n",
    "ax1.axvline(x=median_cn, color='green', linestyle='-', label='Median')\n",
    "\n",
    "ax1.set(xlabel='Grades', ylabel='Number of candidates', title='NU_NOTA_CN: Natural Sciences grade')\n",
    "for label in ax1.get_xticklabels():\n",
    "    label.set_rotation(20)\n",
    "ax1.legend()\n",
    "ax1.grid(False)\n",
    "\n",
    "# Human Sciences\n",
    "ax2 = ax[1]\n",
    "sns.histplot(x=df_EDA['NU_NOTA_CH'], bins=int(180/5), color='yellow', edgecolor='black', ax=ax2, alpha=0.2)\n",
    "mean_cn = df_EDA['NU_NOTA_CH'].mean()\n",
    "median_cn = df_EDA['NU_NOTA_CH'].median()\n",
    "ax2.axvline(x=mean_cn, color='red', linestyle='--', label='Mean')\n",
    "ax2.axvline(x=median_cn, color='green', linestyle='-', label='Median')\n",
    "\n",
    "ax2.set(xlabel='Grades', ylabel='Number of candidates', title='NU_NOTA_CH: Human Sciences grade')\n",
    "for label in ax2.get_xticklabels():\n",
    "    label.set_rotation(20)\n",
    "ax2.legend()\n",
    "ax2.grid(False)\n",
    "\n",
    "# Math\n",
    "ax3 = ax[2]\n",
    "sns.histplot(x=df_EDA['NU_NOTA_MT'], bins=int(180/5), color='pink', edgecolor='black', ax=ax3, alpha=0.2)\n",
    "mean_cn = df_EDA['NU_NOTA_MT'].mean()\n",
    "median_cn = df_EDA['NU_NOTA_MT'].median()\n",
    "ax3.axvline(x=mean_cn, color='red', linestyle='--', label='Mean')\n",
    "ax3.axvline(x=median_cn, color='green', linestyle='-', label='Median')\n",
    "ax3.set(xlabel='Grades', ylabel='Number of candidates', title='NU_NOTA_MT: Mathematics grade')\n",
    "for label in ax3.get_xticklabels():\n",
    "    label.set_rotation(20)\n",
    "ax3.legend()\n",
    "ax3.grid(False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "# Languages\n",
    "ax4 = ax[0]\n",
    "sns.histplot(x=df_EDA['NU_NOTA_LC'], bins=int(180/5), color='green', edgecolor='black', ax=ax4, alpha=0.2)\n",
    "mean_cn = df_EDA['NU_NOTA_LC'].mean()\n",
    "median_cn = df_EDA['NU_NOTA_LC'].median()\n",
    "ax4.axvline(x=mean_cn, color='red', linestyle='--', label='Mean')\n",
    "ax4.axvline(x=median_cn, color='green', linestyle='-', label='Median')\n",
    "ax4.set(xlabel='Grades', ylabel='Number of candidates', title='NU_NOTA_LC: Language Abilities grade')\n",
    "for label in ax4.get_xticklabels():\n",
    "    label.set_rotation(20)\n",
    "ax4.legend()\n",
    "ax4.grid(False)\n",
    "\n",
    "\n",
    "# Natural Sciences\n",
    "ax5 = ax[1]\n",
    "sns.histplot(x=df_EDA['NU_NOTA_REDACAO'], bins=int(180/5), color='purple', edgecolor='black', ax=ax5, alpha=0.2)\n",
    "mean_cn = df_EDA['NU_NOTA_REDACAO'].mean()\n",
    "median_cn = df_EDA['NU_NOTA_REDACAO'].median()\n",
    "ax5.axvline(x=mean_cn, color='red', linestyle='--', label='Mean')\n",
    "ax5.axvline(x=median_cn, color='green', linestyle='-', label='Median')\n",
    "\n",
    "ax5.set(xlabel='Grades', ylabel='Number of candidates', title='NU_NOTA_REDACAO: Critical essay grade')\n",
    "for label in ax5.get_xticklabels():\n",
    "    label.set_rotation(20)\n",
    "ax5.legend()\n",
    "ax5.grid(False)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WezTqJhT-SW"
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong><div style=\"color: rgb(0, 0, 0);\">ðŸ“Œ  Insights:</div></strong> <br>\n",
    "<div style=\"color: rgb(0, 0, 0);\">â†’ The highest mean was for Human Sciences, which more candidates concentrating in the right tail;</div>\n",
    "<div style=\"color: rgb(0, 0, 0);\">â†’ Natural Sciences and Maths appear to have been the toughest subjects, with most of the candidates having a smaller grade, a bigger left tail;</div>\n",
    "<div style=\"color: rgb(0, 0, 0);\">â†’ The Languages grade distribution looks a lot like a normal distribution;</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_normality_ks_shapiro(\n",
    "    df: pd.DataFrame,\n",
    "    columns: List[str],\n",
    "    alpha: float = 0.05\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs Kolmogorovâ€“Smirnov and Shapiroâ€“Wilk normality tests\n",
    "    for each column and returns a consolidated table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset with numeric columns.\n",
    "    columns : List[str]\n",
    "        Columns to test.\n",
    "    alpha : float\n",
    "        Significance level.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Table summarizing normality test results.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for col in columns:\n",
    "        data = df[col].dropna()\n",
    "\n",
    "        # Fit mean and std for KS\n",
    "        mean, std = data.mean(), data.std()\n",
    "\n",
    "        # Kolmogorovâ€“Smirnov test\n",
    "        ks_stat, ks_p = kstest(data, \"norm\", args=(mean, std))\n",
    "\n",
    "        # Shapiroâ€“Wilk (limit 5000 samples)\n",
    "        if len(data) > 5000:\n",
    "            data_sample = data.sample(5000, random_state=42)\n",
    "        else:\n",
    "            data_sample = data\n",
    "\n",
    "        sh_stat, sh_p = shapiro(data_sample)\n",
    "\n",
    "        results.append({\n",
    "            \"column\": col,\n",
    "            \"ks_stat\": ks_stat,\n",
    "            \"ks_p\": ks_p,\n",
    "            \"ks_normal\": ks_p > alpha,\n",
    "            \"shapiro_stat\": sh_stat,\n",
    "            \"shapiro_p\": sh_p,\n",
    "            \"shapiro_normal\": sh_p > alpha,\n",
    "            \"normal_consensus\": (ks_p > alpha) and (sh_p > alpha)\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"Consolidated Normality Test Results (KS + Shapiro)\")\n",
    "    print(results_df)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normality_distribution(\n",
    "    df: pd.DataFrame,\n",
    "    columns: List[str],\n",
    "    bins: int = 40\n",
    ") -> Figure:\n",
    "    \"\"\"\n",
    "    Plots histogram + KDE + fitted normal curve for each column.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(columns)\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(12, 4 * n))\n",
    "\n",
    "    # Handle case where n == 1\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, col in zip(axes, columns):\n",
    "        data = df[col].dropna()\n",
    "        mean, std = data.mean(), data.std()\n",
    "\n",
    "        # Histogram + KDE\n",
    "        sns.histplot(\n",
    "            data, bins=bins, kde=True, stat=\"density\",\n",
    "            color=\"#4A6FA5\", alpha=0.6, ax=ax\n",
    "        )\n",
    "\n",
    "        # Fitted Normal Curve\n",
    "        x = np.linspace(data.min(), data.max(), 300)\n",
    "        y = norm.pdf(x, mean, std)\n",
    "        ax.plot(x, y, color=\"red\", linestyle=\"--\", linewidth=2,\n",
    "                label=f\"Normal Fit (mean={mean:.2f}, stddev={std:.2f})\")\n",
    "\n",
    "        ax.set_title(f\"Distribution of {col}\", fontsize=15)\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_qq(\n",
    "    df: pd.DataFrame,\n",
    "    columns: List[str]\n",
    ") -> Figure:\n",
    "    \"\"\"\n",
    "    Creates QQ-plots for each selected column.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(columns)\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(10, 4*n))\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, col in zip(axes, columns):\n",
    "        data = df[col].dropna()\n",
    "\n",
    "        probplot(data, dist=\"norm\", plot=ax)\n",
    "        ax.set_title(f\"QQ-Plot for {col}\", fontsize=15)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_normality_ks_shapiro(df, ['NU_NOTA_CN','NU_NOTA_CH','NU_NOTA_LC','NU_NOTA_MT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normality_distribution(df, ['NU_NOTA_CN','NU_NOTA_CH','NU_NOTA_LC','NU_NOTA_MT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qq(df, ['NU_NOTA_CN','NU_NOTA_CH','NU_NOTA_LC','NU_NOTA_MT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INcXHNI3Sj5U"
   },
   "outputs": [],
   "source": [
    "# Creating columns for average grade\n",
    "df_EDA[\"MEAN_GRADES\"] = df_EDA[['NU_NOTA_CN', 'NU_NOTA_CH','NU_NOTA_LC','NU_NOTA_MT','NU_NOTA_REDACAO']].mean(axis=1)\n",
    "df_private = df_EDA[df_EDA[\"TP_ESCOLA\"]==\"Private\"]\n",
    "df_public = df_EDA[df_EDA[\"TP_ESCOLA\"]==\"Public\"]\n",
    "\n",
    "# Plotting histogram\n",
    "plt.figure(figsize=(15, 6))\n",
    "ax = sns.histplot(data=df_public,x='MEAN_GRADES', bins=int(180/5), color='blue', edgecolor='black', label='Public')\n",
    "ax = sns.histplot(data=df_private,x='MEAN_GRADES', bins=int(180/5), color='lightblue', edgecolor='black',label='Private')\n",
    "ax.set(xlabel='Grades', ylabel='Number of candidates', title='Distribution of mean grades for private and public schools')\n",
    "ax.legend()\n",
    "# plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified column for school type.\n",
    "df[\"type_school\"] = df[\"TP_ESCOLA\"].map({\n",
    "    1: \"Public\",\n",
    "    2: \"Private\",\n",
    "    3: \"Foreign\"\n",
    "})\n",
    "\n",
    "# If there are missing values, fill in as \"Public\"\n",
    "df[\"type_school\"] = df[\"type_school\"].fillna(\"Public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_income(v:int):\n",
    "    '''\n",
    "    Return the group according to the income bracket.\n",
    "    '''\n",
    "    # Q006 is family income in the ENEM exam (ascending order).\n",
    "    if v == 3:\n",
    "        return \"Low\"\n",
    "    elif v == 6:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "df[\"group_income\"] = df[\"Q006\"].astype('category').cat.codes.apply(categorize_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_env(row):\n",
    "    good_conditions = 0\n",
    "\n",
    "    # Internet (Q025)\n",
    "    if row[\"Q025\"] in [\"B\"]:\n",
    "        good_conditions += 1\n",
    "\n",
    "    # Number of residents (Q005)\n",
    "    if row[\"Q005\"] <= 4:\n",
    "        good_conditions += 1\n",
    "\n",
    "    # Bathroom (Q006)\n",
    "    if row[\"Q011\"] in [\"D\", \"E\"]:\n",
    "        good_conditions += 1\n",
    "\n",
    "    return \"Adequate\" if good_conditions >= 2 else \"Precarious\"\n",
    "\n",
    "df[\"study_env\"] = df.apply(study_env, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "PALETTE = {\n",
    "    \"Precarious\": \"#A7ADB7\",\n",
    "    \"Adequate\": \"#3C4AFF\"\n",
    "}\n",
    "\n",
    "\n",
    "df[\"school_income_group\"] = (\n",
    "    df[\"type_school\"] + \" / \" + df[\"group_income\"]\n",
    ")\n",
    "\n",
    "grupos_desejados = [\"Public / Low\", \"Private / High\"]\n",
    "df_plot = df[df[\"school_income_group\"].isin(grupos_desejados)].copy()\n",
    "\n",
    "resumo = (\n",
    "    df_plot\n",
    "    .groupby([\"school_income_group\", \"study_env\"])[\"NU_NOTA_MT\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "resumo[\"school_income_group\"] = pd.Categorical(\n",
    "    resumo[\"school_income_group\"],\n",
    "    categories=[\"Public / Low\", \"Private / High\"],\n",
    "    ordered=True\n",
    ")\n",
    "resumo[\"study_env\"] = pd.Categorical(\n",
    "    resumo[\"study_env\"],\n",
    "    categories=[\"Precarious\", \"Adequate\"],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "\n",
    "bars = sns.barplot(\n",
    "    data=resumo,\n",
    "    y=\"school_income_group\",\n",
    "    x=\"NU_NOTA_MT\",\n",
    "    hue=\"study_env\",\n",
    "    palette=PALETTE,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.2,\n",
    "    ax=ax,\n",
    "    orient=\"h\"\n",
    ")\n",
    "\n",
    "# ---- AnotaÃ§Ãµes elegantes nas barras ----\n",
    "for p in bars.patches:\n",
    "    width = p.get_width()\n",
    "    ax.annotate(\n",
    "        f\"{width:.1f}\",\n",
    "        (width, p.get_y() + p.get_height() / 2),\n",
    "        ha=\"left\",\n",
    "        va=\"center\",\n",
    "        fontsize=13,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"#252525\",\n",
    "        xytext=(8, 0),\n",
    "        textcoords=\"offset points\"\n",
    "    )\n",
    "\n",
    "\n",
    "ax.add_patch(Rectangle(\n",
    "    (0, -0.5),\n",
    "    max(resumo[\"NU_NOTA_MT\"]) * 1.05,\n",
    "    3,\n",
    "    color=\"#F5F6F8\",\n",
    "    alpha=0.35,\n",
    "    zorder=-1\n",
    "))\n",
    "\n",
    "\n",
    "ax.set_title(\n",
    "    \"Effect of Home Study Environment on Math Grades\",\n",
    "    fontsize=20, weight=\"bold\", pad=18\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Average Math Grade (NU_NOTA_MT)\", fontsize=18)\n",
    "ax.set_ylabel(\"High School Type / Socioeconomic Level\", fontsize=18)\n",
    "\n",
    "plt.xlim(0, max(resumo[\"NU_NOTA_MT\"])*1.15)\n",
    "\n",
    "plt.legend(\n",
    "    title=\"Study Environment\",\n",
    "    title_fontsize=13,\n",
    "    fontsize=13,\n",
    "    frameon=False,\n",
    "    loc=\"upper right\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q006 is already an ordinal number (A, B, C...), let's map it to an order.\n",
    "ordem_q006 = sorted(df[\"Q006\"].dropna().unique())\n",
    "map_q006 = {v: i for i, v in enumerate(ordem_q006)}\n",
    "df[\"income_ord\"] = df[\"Q006\"].map(map_q006)\n",
    "\n",
    "# Average grade by income level (order of Q006)\n",
    "resumo_renda_mt = (\n",
    "    df.groupby(\"income_ord\")[\"NU_NOTA_MT\"]\n",
    "      .agg([\"mean\", \"count\"])\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(11, 6))\n",
    "ax = sns.lineplot(\n",
    "    data=resumo_renda_mt,\n",
    "    x=\"income_ord\",\n",
    "    y=\"mean\",\n",
    "    marker=\"o\",\n",
    "    linewidth=2.5\n",
    ")\n",
    "plt.title(\"Average Grade in Mathematics by Family Income Bracket (Q006)\", fontsize=18, weight=\"bold\")\n",
    "plt.xlabel(\"Income Bracket (Q006 order)\")\n",
    "plt.ylabel(\"Average Grade in Mathematics\")\n",
    "\n",
    "# highlight points with text.\n",
    "for _, row in resumo_renda_mt.iterrows():\n",
    "    ax.annotate(\n",
    "        f\"{row['mean']:.0f}\",\n",
    "        (row[\"income_ord\"], row[\"mean\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 6),\n",
    "        ha=\"center\",\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df[[\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\"NU_NOTA_MT\",\"income_ord\"]].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9toky6cQjc3"
   },
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df_public, df_private], ignore_index=True)\n",
    "plt.figure(figsize=(15, 6))\n",
    "ax = sns.countplot(data=combined_df,x='Q006',hue='TP_ESCOLA',palette=['blue',\"lightblue\"])\n",
    "ax.set(xlabel='Family income classes', ylabel='Number of candidates', title='Amount of candidates from public/private school in each family income class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMJv22v0MPnc"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "classes = df_EDA[\"Q006\"].unique().tolist()\n",
    "mean_grades = df_EDA.groupby([\"Q006\"])['MEAN_GRADES'].mean().tolist()\n",
    "plt.bar(classes, mean_grades)\n",
    "\n",
    "ax.set_ylabel('Number of candidates')\n",
    "ax.set_xlabel('Income class')\n",
    "ax.set_title('Mean grade for all candidates from every family income class')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRtvQuDNT2iX"
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong><div style=\"color: rgb(0, 0, 0);\">ðŸ“Œ  Insights:</div></strong> <br>\n",
    "<div style=\"color: rgb(0, 0, 0);\">â†’ The distriution of grades of candidates from private schools are more to the left than for public schools;</div>\n",
    "<div style=\"color: rgb(0, 0, 0);\">â†’ The candidates that belong to lower family income classes also mostly go to public schools, while higer income classes go to private school. That is also common knowledge nowadays in Brazil, unfortunately, since the private school tend to offer a better education than public ones.;</div>\n",
    "<div style=\"color: rgb(0, 0, 0);\">â†’ The higher the family income class, the higher the candidates mean grades;</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5PUZYWwnepN"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUgQfsIwUQvC"
   },
   "source": [
    "Now that we already did the Exploratory Data Analysis, we should **drop features that may lead to a biased model**. The features indicating **sex** and **ethnicity** are typically the ones we drop in that case.\n",
    "\n",
    "We know that someone's ethnicity shouldn't influence their grade in an exam per se. If we see a correlation, it shows characteristics of structural racism, in which ethnic minorities have less opportunities, in our case, that is reflected, for example, on the family income. So we will drop these two features. That way we can leave our model to focus on other features that provide more causation to the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGXHxh1PUYpM"
   },
   "outputs": [],
   "source": [
    "df = df.drop(['TP_COR_RACA', 'TP_SEXO', 'SG_UF_ESC', 'TP_ST_CONCLUSAO','NU_NOTA_REDACAO', 'type_school', 'group_income', 'study_env', 'school_income_group', 'income_ord'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXSG2zsFQUum"
   },
   "source": [
    "## Handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbe5IF_RQW6D"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsbKotTsQW8q"
   },
   "outputs": [],
   "source": [
    "print(\"Missing values in the dataset:\",df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXSg_t7GP0Zc"
   },
   "source": [
    "## Handling *outliers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySyRu0amiF_N"
   },
   "outputs": [],
   "source": [
    "def remove_outliers_zscore(df, columns, threshold=3):\n",
    "    \"\"\"\n",
    "    Removes outliers from specified columns using the Z-score method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataset.\n",
    "    columns : list\n",
    "        List of numeric columns to apply Z-score filtering.\n",
    "    threshold : float\n",
    "        Z-score threshold to define an outlier (default = 3).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "\n",
    "    df_clean = df.copy()\n",
    "    summary = []\n",
    "\n",
    "    for col in columns:\n",
    "        series = df_clean[col]\n",
    "\n",
    "        # Compute z-scores\n",
    "        mean = series.mean()\n",
    "        std = series.std()\n",
    "\n",
    "        z_scores = (series - mean) / std\n",
    "\n",
    "        # Mask for values within threshold\n",
    "        mask = z_scores.abs() <= threshold\n",
    "\n",
    "        removed_count = (~mask).sum()\n",
    "        removed_percent = round(100 * removed_count / len(series), 2)\n",
    "\n",
    "        # Quartiles BEFORE removal\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "\n",
    "        summary.append({\n",
    "            \"column\": col,\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"Q1\": Q1,\n",
    "            \"Q3\": Q3,\n",
    "            \"removed_rows\": removed_count,\n",
    "            \"removed_percentage\": removed_percent\n",
    "        })\n",
    "\n",
    "        # Apply filtering\n",
    "        df_clean = df_clean[mask]\n",
    "\n",
    "    # Print summary table\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "\n",
    "    print(\"\\n===== Z-Score Outlier Removal Summary =====\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(f\"\\nTotal remaining rows: {len(df_clean)}\")\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8hO1G2GJmda"
   },
   "outputs": [],
   "source": [
    "def remove_outliers_IQR(df, columns, verbose=True):\n",
    "    \"\"\"\n",
    "    Removes outliers from a DataFrame using the IQR rule.\n",
    "    Works for one or multiple numeric columns.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataset.\n",
    "    columns : list\n",
    "        List of numeric columns from which outliers will be removed.\n",
    "    verbose : bool\n",
    "        If True, prints a summary of removed outliers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Cleaned DataFrame with outliers removed.\n",
    "    pd.DataFrame\n",
    "        Summary table of outliers removed per column.\n",
    "    \"\"\"\n",
    "\n",
    "    df_clean = df.copy()\n",
    "    summary = []\n",
    "\n",
    "    for col in columns:\n",
    "        series = df_clean[col].dropna()\n",
    "\n",
    "        # Quartiles & IQR\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Boolean mask of non-outliers\n",
    "        mask = (df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)\n",
    "\n",
    "        removed = len(df_clean) - mask.sum()\n",
    "\n",
    "        summary.append({\n",
    "            \"column\": col,\n",
    "            \"Q1\": Q1,\n",
    "            \"Q3\": Q3,\n",
    "            \"IQR\": IQR,\n",
    "            \"lower_bound\": lower_bound,\n",
    "            \"upper_bound\": upper_bound,\n",
    "            \"removed_rows\": removed,\n",
    "            \"removed_percentage\": round(100 * removed / len(df), 2)\n",
    "        })\n",
    "\n",
    "        # Keep only non-outliers\n",
    "        df_clean = df_clean[mask]\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n===== Outlier Removal Summary =====\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "        print(f\"\\nTotal removed rows: {len(df) - len(df_clean)}\")\n",
    "        print(f\"Final dataset size: {len(df_clean)} rows\")\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDUxU6ObJmnt"
   },
   "outputs": [],
   "source": [
    "df = remove_outliers_IQR(df, ['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC','NU_NOTA_MT'], verbose=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WepgwAuUkVoz"
   },
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPuRO8hCJm5S"
   },
   "outputs": [],
   "source": [
    "# Transforming categorical data to numeric\n",
    "for i in range(1,28):\n",
    "    if i < 10:\n",
    "        name = f\"Q00{i}\"\n",
    "    elif i == 26:\n",
    "        continue\n",
    "    else:\n",
    "        name = f\"Q0{i}\"\n",
    "    df[name] = df[name].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZeSr2kLnmB05",
    "outputId": "6611e1d4-ddfc-421c-b002-2c67edb72a3c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(df[['TP_FAIXA_ETARIA', 'TP_ESTADO_CIVIL', 'TP_ESCOLA', 'TP_ENSINO',\n",
    "       'TP_LOCALIZACAO_ESC','NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC',\n",
    "       'NU_NOTA_MT','Q001', 'Q002', 'Q003', 'Q004', 'Q005', 'Q006',\n",
    "       'Q007', 'Q008', 'Q009', 'Q010', 'Q011', 'Q012', 'Q013', 'Q014', 'Q015',\n",
    "       'Q016', 'Q017', 'Q018', 'Q019', 'Q020', 'Q021', 'Q022', 'Q023', 'Q024',\n",
    "       'Q025', 'Q027']].corr(), annot=False, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivUF56Bmp_b-"
   },
   "source": [
    "## Division into Training and Testing bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDsW3tx4mB-_"
   },
   "outputs": [],
   "source": [
    "X = df.drop(['NU_NOTA_CN','NU_NOTA_CH','NU_NOTA_LC','NU_NOTA_MT'], axis=1)\n",
    "y = df[['NU_NOTA_CN','NU_NOTA_CH','NU_NOTA_LC','NU_NOTA_MT']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HjtlF_s9JnfS",
    "outputId": "389c6f6a-ae97-45db-eb45-7b9ae4b9f324"
   },
   "outputs": [],
   "source": [
    "print(f'X_train size: {X_train.shape}')\n",
    "print(f'y_train size: {y_train.shape}')\n",
    "print(f'X_test size: {X_test.shape}')\n",
    "print(f'y_test size: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzH8y6VVnerp"
   },
   "source": [
    "# Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1fkmqSIowyv"
   },
   "outputs": [],
   "source": [
    "def evaluate_multioutput_regression(y_true, y_pred, target_names=None):\n",
    "    \"\"\"\n",
    "    Evaluates a multi-output regression model using RMSE, MAE, and RÂ²\n",
    "    for each target column and returns a summary table.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True target values (n_samples, n_outputs)\n",
    "    y_pred : array-like\n",
    "        Predicted target values (n_samples, n_outputs)\n",
    "    target_names : list or None\n",
    "        List of target/column names (optional)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Evaluation metrics per target and overall averages.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    n_outputs = y_true.shape[1]\n",
    "\n",
    "    if target_names is None:\n",
    "        target_names = [f\"target_{i}\" for i in range(n_outputs)]\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for i in range(n_outputs):\n",
    "        rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))\n",
    "        mae = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        r2 = r2_score(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "        metrics.append({\n",
    "            \"target\": target_names[i],\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R2\": r2\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(metrics)\n",
    "\n",
    "    # Add averages\n",
    "    avg_row = {\n",
    "        \"target\": \"AVERAGE\",\n",
    "        \"RMSE\": df_results[\"RMSE\"].mean(),\n",
    "        \"MAE\": df_results[\"MAE\"].mean(),\n",
    "        \"R2\": df_results[\"R2\"].mean()\n",
    "    }\n",
    "\n",
    "    df_results = pd.concat([df_results, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "    print(\"\\n===== Model Evaluation Summary =====\")\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2xf0iAzsqEu"
   },
   "outputs": [],
   "source": [
    "def plot_multioutput_diagnostics(y_true, y_pred, target_names):\n",
    "    \"\"\"\n",
    "    Generates diagnostic plots for each target:\n",
    "    1. Actual vs Predicted\n",
    "    2. Residual distribution\n",
    "    3. Residual boxplot\n",
    "    4. Homoscedasticity check: residuals vs predicted + LOWESS smoothing\n",
    "    \"\"\"\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Garantir formato 2D\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "    n_outputs = y_true.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(4, n_outputs, figsize=(6 * n_outputs, 22))\n",
    "\n",
    "    # Se sÃ³ tiver 1 coluna, axes vem 1D -> convertemos para 2D [4,1]\n",
    "    if n_outputs == 1:\n",
    "        axes = np.array(axes).reshape(4, 1)\n",
    "\n",
    "    for i in range(n_outputs):\n",
    "        true_vals = y_true[:, i]\n",
    "        pred_vals = y_pred[:, i]\n",
    "        residuals = true_vals - pred_vals\n",
    "\n",
    "        # 1) Actual vs Predicted\n",
    "        ax1 = axes[0, i]\n",
    "        sns.scatterplot(x=true_vals, y=pred_vals, s=30, ax=ax1)\n",
    "        ax1.plot(\n",
    "            [true_vals.min(), true_vals.max()],\n",
    "            [true_vals.min(), true_vals.max()],\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\"\n",
    "        )\n",
    "        ax1.set_title(f\"Actual vs Predicted â€“ {target_names[i]}\", fontsize=14, fontweight=\"bold\")\n",
    "        ax1.set_xlabel(\"Actual\")\n",
    "        ax1.set_ylabel(\"Predicted\")\n",
    "\n",
    "        # 2) Residual distribution\n",
    "        ax2 = axes[1, i]\n",
    "        sns.histplot(residuals, kde=True, ax=ax2)\n",
    "        ax2.set_title(f\"Residual Distribution â€“ {target_names[i]}\", fontsize=14, fontweight=\"bold\")\n",
    "        ax2.set_xlabel(\"Residuals\")\n",
    "\n",
    "        # 3) Residual boxplot\n",
    "        ax3 = axes[2, i]\n",
    "        sns.boxplot(x=residuals, ax=ax3)\n",
    "        ax3.set_title(f\"Residual Boxplot â€“ {target_names[i]}\", fontsize=14, fontweight=\"bold\")\n",
    "        ax3.set_xlabel(\"Residuals\")\n",
    "\n",
    "        # 4) Homoscedasticity Plot (Residuals vs Predicted)\n",
    "        ax4 = axes[3, i]\n",
    "        sns.scatterplot(x=pred_vals, y=residuals, s=25, alpha=0.5, ax=ax4)\n",
    "        sns.regplot(x=pred_vals, y=residuals, scatter=False, ax=ax4, color=\"red\")\n",
    "        ax4.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "        ax4.set_title(f\"Homoscedasticity Check â€“ {target_names[i]}\", fontsize=14, fontweight=\"bold\")\n",
    "        ax4.set_xlabel(\"Predicted\")\n",
    "        ax4.set_ylabel(\"Residuals\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UE0OlzkN9REX"
   },
   "outputs": [],
   "source": [
    "def get_feature_names(model, X_train):\n",
    "    \"\"\"\n",
    "    Extracts feature names for models inside a Pipeline or raw estimators.\n",
    "    Works with:\n",
    "    - Pipeline(StandardScaler / OneHotEncoder / etc. + model)\n",
    "    - Raw Scikit-learn models\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"named_steps\"):\n",
    "        # If model is inside a Pipeline\n",
    "        preprocessor = model.named_steps.get(\"scaler\") or \\\n",
    "                       model.named_steps.get(\"preprocessor\")\n",
    "\n",
    "        if preprocessor and hasattr(preprocessor, \"get_feature_names_out\"):\n",
    "            return preprocessor.get_feature_names_out()\n",
    "\n",
    "    # Otherwise assume X_train is DataFrame\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        return X_train.columns.values\n",
    "\n",
    "    return np.array([f\"feature_{i}\" for i in range(X_train.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCatgZZ-9RKM"
   },
   "outputs": [],
   "source": [
    "def shap_feature_importance(\n",
    "    model,\n",
    "    X,\n",
    "    target_names=None,\n",
    "    max_samples=3000,\n",
    "    random_state=42,\n",
    "    top_n=15,\n",
    "    figsize=(10, 6),\n",
    "    plot=True,\n",
    "    plot_per_target=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes SHAP feature importance for multi-output models, returning:\n",
    "\n",
    "    - Global importance across outputs\n",
    "    - Importance per output (target)\n",
    "    - Plots for each individual target (optional)\n",
    "    - Global plot (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure DataFrame for feature names\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    # Sampling to reduce SHAP computation cost\n",
    "    if len(X) > max_samples:\n",
    "        X_sample = X.sample(max_samples, random_state=random_state)\n",
    "    else:\n",
    "        X_sample = X.copy()\n",
    "\n",
    "    feature_names = X_sample.columns.tolist()\n",
    "\n",
    "    # Detect multioutput\n",
    "    is_multioutput = hasattr(model, \"estimators_\")\n",
    "\n",
    "    if is_multioutput:\n",
    "        n_targets = len(model.estimators_)\n",
    "    else:\n",
    "        y_sample = np.asarray(model.predict(X_sample.iloc[:5]))\n",
    "        n_targets = 1 if y_sample.ndim == 1 else y_sample.shape[1]\n",
    "\n",
    "    # Define target names\n",
    "    if target_names is None:\n",
    "        target_names = [f\"target_{i}\" for i in range(n_targets)]\n",
    "    else:\n",
    "        assert len(target_names) == n_targets, \"Mismatch in number of target names.\"\n",
    "\n",
    "    per_target_importance = {}\n",
    "    overall_accumulator = np.zeros(len(feature_names))\n",
    "\n",
    "    \n",
    "    # shap for each output\n",
    "    for i in range(n_targets):\n",
    "\n",
    "        if is_multioutput:\n",
    "            # One estimator per output\n",
    "            est = model.estimators_[i]\n",
    "            explainer = shap.Explainer(est, X_sample)\n",
    "            shap_values = explainer(X_sample)\n",
    "        else:\n",
    "            # Single model with vector output â†’ wrapper\n",
    "            def predict_i(Xarr, estimator=model, idx=i):\n",
    "                ypred = estimator.predict(Xarr)\n",
    "                ypred = np.asarray(ypred)\n",
    "                if ypred.ndim == 1:\n",
    "                    return ypred\n",
    "                return ypred[:, idx]\n",
    "\n",
    "            explainer = shap.Explainer(predict_i, X_sample)\n",
    "            shap_values = explainer(X_sample)\n",
    "\n",
    "        values = shap_values.values  # shape = (n_samples, n_features)\n",
    "        mean_abs = np.abs(values).mean(axis=0)  # 1D vector\n",
    "\n",
    "        overall_accumulator += mean_abs\n",
    "\n",
    "        # Per-output DataFrame\n",
    "        df_imp = (\n",
    "            pd.DataFrame({\n",
    "                \"feature\": feature_names,\n",
    "                \"mean_abs_shap\": mean_abs\n",
    "            })\n",
    "            .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        per_target_importance[target_names[i]] = df_imp\n",
    "\n",
    "        # plot per output\n",
    "        if plot and plot_per_target:\n",
    "\n",
    "            top_df = df_imp.head(top_n).iloc[::-1]  # reverse for barh order\n",
    "\n",
    "            plt.figure(figsize=figsize)\n",
    "            plt.barh(top_df[\"feature\"], top_df[\"mean_abs_shap\"])\n",
    "            plt.xlabel(\"Mean |SHAP value|\")\n",
    "            plt.ylabel(\"Feature\")\n",
    "            plt.title(f\"SHAP Feature Importance â€” {target_names[i]}\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "    # global importance - mean across output\n",
    "    overall_mean = overall_accumulator / n_targets\n",
    "\n",
    "    overall_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": feature_names,\n",
    "            \"mean_abs_shap\": overall_mean\n",
    "        })\n",
    "        .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # global plot\n",
    "    if plot:\n",
    "        top = overall_df.head(top_n).iloc[::-1]\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.barh(top[\"feature\"], top[\"mean_abs_shap\"])\n",
    "        plt.xlabel(\"Mean |SHAP value| (average across outputs)\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(\"Global SHAP Feature Importance (Across All Outputs)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return overall_df, per_target_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_MbTayPuWma"
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sti96EwV4S52",
    "outputId": "4919b69c-c119-40c0-c06f-554aff2a51d0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Base Lasso model\n",
    "lasso_base = Lasso(random_state=42, max_iter=10000)\n",
    "model = MultiOutputRegressor(lasso_base)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"estimator__alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Halving Grid Search\n",
    "halving_search = HalvingGridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    factor=3,    # how aggressively to eliminate weak models\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "halving_search.fit(X_train, y_train)\n",
    "\n",
    "# Best params\n",
    "print(\"Best parameters:\", halving_search.best_params_)\n",
    "print(\"Best score (neg MSE):\", halving_search.best_score_)\n",
    "\n",
    "best_rmse = np.sqrt(-halving_search.best_score_)\n",
    "print(\"Best CV RMSE:\", best_rmse)\n",
    "\n",
    "# Best model predictions\n",
    "best_lasso = halving_search.best_estimator_\n",
    "y_pred = best_lasso.predict(X_test)\n",
    "\n",
    "print('Lasso Regression training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "BznWCxVSuyMu",
    "outputId": "e8713c07-4bb3-4d15-be2a-b9deb0743d94"
   },
   "outputs": [],
   "source": [
    "evaluate_multioutput_regression(y_test, y_pred, target_names=y_test.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KEwpDLyisqQ-",
    "outputId": "da76487f-fc0d-4e51-be28-6b8191aa2c5d"
   },
   "outputs": [],
   "source": [
    "plot_multioutput_diagnostics(y_test, y_pred, target_names=y_test.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T8hjuAUD9tkT",
    "outputId": "59a36084-45f2-4d0b-a481-4f99e7096693"
   },
   "outputs": [],
   "source": [
    "shap_feature_importance(best_model, X_test, y_test.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms-AYxH5vsZQ"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IE10--7CvuzC",
    "outputId": "aa1957c5-8b6b-4b00-c0fa-f489d8d61612"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Base model\n",
    "rf_base = RandomForestRegressor(\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model = MultiOutputRegressor(rf_base)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"estimator__n_estimators\": [200, 400, 800],\n",
    "    \"estimator__max_depth\": [5, 10, 15],\n",
    "    \"estimator__min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Halving Grid Search\n",
    "rf_search = HalvingGridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    factor=3,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RF params:\", rf_search.best_params_)\n",
    "print(\"Best RF score (neg MSE):\", rf_search.best_score_)\n",
    "print(\"Best RF RMSE:\", np.sqrt(-rf_search.best_score_))\n",
    "\n",
    "best_rf = rf_search.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "pUzMTNeCvu76",
    "outputId": "d6ace7e7-3a16-41c4-85c9-c02c83bc47ef"
   },
   "outputs": [],
   "source": [
    "evaluate_multioutput_regression(y_test, y_pred, target_names=y_test.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "id": "dPMPuac6vvD5",
    "outputId": "f44dfd22-e30c-4550-b935-c86004685732"
   },
   "outputs": [],
   "source": [
    "plot_multioutput_diagnostics(y_test, y_pred, target_names=y_test.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iZV0XievvMv"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QE3eSzjwDWp",
    "outputId": "d83db968-b519-4417-af24-c8634ea11b0b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Base model\n",
    "xgb_base = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model = MultiOutputRegressor(xgb_base)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"estimator__n_estimators\": [300, 600, 900],\n",
    "    \"estimator__max_depth\": [4, 6, 8],\n",
    "    \"estimator__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"estimator__subsample\": [0.7, 0.9, 1.0],\n",
    "    \"estimator__colsample_bytree\": [0.7, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Halving Grid Search\n",
    "xgb_search = HalvingGridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    factor=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best XGBoost params:\", xgb_search.best_params_)\n",
    "print(\"Best XGBoost score:\", xgb_search.best_score_)\n",
    "print(\"Best XGB RMSE:\", np.sqrt(-xgb_search.best_score_))\n",
    "\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "print(\"XGBoost training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "ApxJVhsDwDfO",
    "outputId": "1c68a28a-c13e-4083-d6a0-19ad656a72ae"
   },
   "outputs": [],
   "source": [
    "evaluate_multioutput_regression(y_test, y_pred, target_names=y_test.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QIFXL7MdwDlO",
    "outputId": "057af3b9-5609-41cc-fa90-2a4d9f11d8c1"
   },
   "outputs": [],
   "source": [
    "plot_multioutput_diagnostics(y_test, y_pred, target_names=y_test.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_select_best_model(\n",
    "    models: List[Tuple[str, Any]],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.DataFrame,\n",
    "    pickle_filename: str = \"best_model.pkl\"\n",
    ") -> Tuple[pd.DataFrame, Any]:\n",
    "    \"\"\"\n",
    "    Evaluates multiple regression models using RMSE, MAE, and R2.\n",
    "    Selects the best model (highest R2), saves it to a pickle file,\n",
    "    and returns both the evaluation table and the best model object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : List[Tuple[str, Any]]\n",
    "        A list of tuples: (model_name, model_instance).\n",
    "    X_train : pd.DataFrame\n",
    "        Training features.\n",
    "    y_train : pd.DataFrame or pd.Series\n",
    "        Training target(s).\n",
    "    X_test : pd.DataFrame\n",
    "        Test features.\n",
    "    y_test : pd.DataFrame or pd.Series\n",
    "        Test target(s).\n",
    "    pickle_filename : str, optional\n",
    "        Filename to save the best model as a pickle, by default \"best_model.pkl\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, Any]\n",
    "        A tuple containing:\n",
    "        - A DataFrame with RMSE, MAE, and R2 for each model\n",
    "        - The best-performing model (highest R2)\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        results.append({\n",
    "            \"model\": name,\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R2\": r2\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nModel Evaluation Results:\\n\")\n",
    "    print(results_df)\n",
    "\n",
    "    # Select the best model based on highest R2\n",
    "    best_row = results_df.loc[results_df[\"R2\"].idxmax()]\n",
    "    best_model_name = best_row[\"model\"]\n",
    "    best_model = dict(models)[best_model_name]\n",
    "\n",
    "    print(f\"\\nBest model: {best_model_name} (R2 = {best_row['R2']:.4f})\")\n",
    "\n",
    "    # Save best model\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(best_model, f)\n",
    "\n",
    "    print(f\"\\nBest model saved to: {pickle_filename}\")\n",
    "\n",
    "    return results_df, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"Lasso\", best_lasso),\n",
    "    (\"Ridge\", best_rf),\n",
    "    (\"RandomForest\", best_xgb)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, best_model = evaluate_and_select_best_model(\n",
    "    models,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    pickle_filename=\"best_regression_baseline_model.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN0fyQUC_wwo"
   },
   "source": [
    "## GNN Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOCIO_COLS: List[str] = [\n",
    "    \"Q001\",\"Q002\",\"Q003\",\"Q004\",\"Q005\",\"Q006\",\n",
    "    \"Q007\",\"Q008\",\"Q009\",\"Q010\",\"Q011\",\"Q012\",\n",
    "    \"Q013\",\"Q014\",\"Q015\",\"Q016\",\"Q017\",\"Q018\",\n",
    "    \"Q019\",\"Q020\",\"Q021\",\"Q022\",\"Q023\",\"Q024\",\n",
    "    \"Q025\",\"Q027\"\n",
    "]\n",
    "\n",
    "TARGET_COLS = [\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\"NU_NOTA_MT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, StandardScaler]:\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Targets\n",
    "    y = df[TARGET_COLS].values.astype(\"float32\")\n",
    "\n",
    "    # Scale targets\n",
    "    scaler_y = StandardScaler()\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "    return df, y_scaled, scaler_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_heterodata(df: pd.DataFrame) -> HeteroData:\n",
    "    data = HeteroData()\n",
    "\n",
    "    num_students = len(df)\n",
    "    data[\"student\"].num_nodes = num_students\n",
    "\n",
    "    # Category nodes and edges\n",
    "    for col in SOCIO_COLS:\n",
    "        categories = df[col].astype(str).unique()\n",
    "        mapping = {cat: i for i, cat in enumerate(categories)}\n",
    "\n",
    "        df[f\"{col}_id\"] = df[col].astype(str).map(mapping)\n",
    "        data[col].num_nodes = len(categories)\n",
    "\n",
    "        src = df.index.values\n",
    "        dst = df[f\"{col}_id\"].values\n",
    "\n",
    "        data[\"student\", f\"has_{col}\", col].edge_index = torch.tensor(\n",
    "            [src, dst], dtype=torch.long\n",
    "        )\n",
    "\n",
    "        data[col, f\"rev_has_{col}\", \"student\"].edge_index = torch.tensor(\n",
    "            [dst, src], dtype=torch.long\n",
    "        )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_knn_edges(df: pd.DataFrame, data: HeteroData, k: int = 5) -> HeteroData:\n",
    "    socio_encoded = df[SOCIO_COLS].apply(\n",
    "        lambda col: col.astype(\"category\").cat.codes).values\n",
    "\n",
    "    sim = cosine_similarity(socio_encoded)\n",
    "    edges = []\n",
    "\n",
    "    num_students = len(df)\n",
    "    for i in range(num_students):\n",
    "        nn_ids = np.argsort(sim[i])[-(k+1):-1]\n",
    "        for j in nn_ids:\n",
    "            edges.append([i, j])\n",
    "\n",
    "    arr = np.array(edges).T\n",
    "    data[\"student\", \"knn\", \"student\"].edge_index = torch.tensor(arr, dtype=torch.long)\n",
    "    data[\"student\", \"knn_rev\", \"student\"].edge_index = torch.tensor(arr[[1, 0], :], dtype=torch.long)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(n: int):\n",
    "    idx = np.arange(n)\n",
    "    idx_train, idx_temp = train_test_split(idx, test_size=0.3, random_state=42)\n",
    "    idx_val, idx_test = train_test_split(idx_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(idx_train, dtype=torch.long),\n",
    "        torch.tensor(idx_val, dtype=torch.long),\n",
    "        torch.tensor(idx_test, dtype=torch.long)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(data: HeteroData, idx_train, idx_val, idx_test, device):\n",
    "\n",
    "    train_loader = NeighborLoader(\n",
    "        data,\n",
    "        input_nodes=(\"student\", idx_train.to(device)),\n",
    "        num_neighbors=[10, 10, 10],\n",
    "        batch_size=512,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader = NeighborLoader(\n",
    "        data,\n",
    "        input_nodes=(\"student\", idx_val.to(device)),\n",
    "        num_neighbors=[10, 10, 10],\n",
    "        batch_size=512,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_loader = NeighborLoader(\n",
    "        data,\n",
    "        input_nodes=(\"student\", idx_test.to(device)),\n",
    "        num_neighbors=[10, 10, 10],\n",
    "        batch_size=512,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENEMHeteroGNNOptimized(nn.Module):\n",
    "    def __init__(self, data: HeteroData, emb_dim=64, hidden_dim=128, out_dim=4, dropout=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            nt: nn.Embedding(data[nt].num_nodes, emb_dim)\n",
    "            for nt in data.node_types\n",
    "        })\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_dim = emb_dim\n",
    "\n",
    "        for _ in range(3):\n",
    "            conv_dict = {et: SAGEConv((in_dim, in_dim), hidden_dim) for et in data.edge_types}\n",
    "            self.convs.append(HeteroConv(conv_dict, aggr=\"sum\"))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        self.norm_student = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data: HeteroData):\n",
    "        device = next(self.parameters()).device\n",
    "        x_dict = {}\n",
    "\n",
    "        for nt in self.embeddings:\n",
    "            if nt in data.node_types and hasattr(data[nt], \"n_id\"):\n",
    "                node_ids = data[nt].n_id.to(device)\n",
    "            else:\n",
    "                node_ids = torch.arange(\n",
    "                    self.embeddings[nt].num_embeddings,\n",
    "                    device=device\n",
    "                )\n",
    "            x_dict[nt] = self.embeddings[nt](node_ids)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, data.edge_index_dict)\n",
    "            x_dict = {nt: self.dropout(F.relu(v)) for nt, v in x_dict.items()}\n",
    "\n",
    "        s = x_dict[\"student\"]\n",
    "        s = self.norm_student(s)\n",
    "        out = self.head(s)\n",
    "\n",
    "        return {\"student\": out}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, model, optimizer, epoch, best_metric, scaler_y):\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"best_metric\": best_metric,\n",
    "        \"scaler_y\": scaler_y\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved in {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, device=\"cpu\"):\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    print(f\"Checkpoint loaded (epoch {ckpt['epoch']}, best RÂ²={ckpt['best_metric']})\")\n",
    "    return ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_resume(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    scaler_y,\n",
    "    checkpoint_path,\n",
    "    max_epochs=120,\n",
    "    patience=20,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ckpt = load_checkpoint(checkpoint_path, model, optimizer, device)\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "        best_metric = ckpt[\"best_metric\"]\n",
    "        scaler_y = ckpt[\"scaler_y\"]\n",
    "        print(f\"Starting from epocch {start_epoch}\")\n",
    "    else:\n",
    "        start_epoch = 1\n",
    "        best_metric = -np.inf\n",
    "        print(\"Training from zero\")\n",
    "\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(start_epoch, max_epochs + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        losses = []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(batch)[\"student\"]\n",
    "            idx_global = batch[\"student\"].n_id\n",
    "            y_batch = y_scaled_tensor[idx_global]\n",
    "\n",
    "            loss = criterion(out, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(losses)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch)[\"student\"]\n",
    "                idx_global = batch[\"student\"].n_id\n",
    "                y_batch = y_scaled_tensor[idx_global]\n",
    "\n",
    "                preds.append(out.cpu().numpy())\n",
    "                trues.append(y_batch.cpu().numpy())\n",
    "\n",
    "        preds = scaler_y.inverse_transform(np.vstack(preds))\n",
    "        trues = scaler_y.inverse_transform(np.vstack(trues))\n",
    "\n",
    "        val_r2 = r2_score(trues, preds, multioutput=\"uniform_average\")\n",
    "\n",
    "        # Check improvement\n",
    "        if val_r2 > best_metric:\n",
    "            best_metric = val_r2\n",
    "            wait = 0\n",
    "            save_checkpoint(checkpoint_path, model, optimizer, epoch, best_metric, scaler_y)\n",
    "            flag = \"\\nBEST\"\n",
    "        else:\n",
    "            wait += 1\n",
    "            flag = \"   \"\n",
    "\n",
    "        print(f\"{flag} Epoch {epoch} | Train Loss={train_loss:.4f} | Val RÂ²={val_r2:.4f}\")\n",
    "\n",
    "        if wait >= patience:\n",
    "            print(\"\\nEarly stopping\")\n",
    "            break\n",
    "\n",
    "    return best_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, scaler_y, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)[\"student\"]\n",
    "            idx_global = batch[\"student\"].n_id\n",
    "            y_batch = y_scaled_tensor[idx_global]\n",
    "\n",
    "            preds.append(out.cpu().numpy())\n",
    "            trues.append(y_batch.cpu().numpy())\n",
    "\n",
    "    preds = scaler_y.inverse_transform(np.vstack(preds))\n",
    "    trues = scaler_y.inverse_transform(np.vstack(trues))\n",
    "\n",
    "    return {\n",
    "        \"RMSE\": mean_squared_error(trues, preds),\n",
    "        \"MAE\": mean_absolute_error(trues, preds),\n",
    "        \"R2\": r2_score(trues, preds)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_student(model, df_row, hetero_data, scaler_y):\n",
    "    model.eval()\n",
    "    idx = df_row.name\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        full_out = model(hetero_data.to(next(model.parameters()).device))[\"student\"]\n",
    "        pred_scaled = full_out[idx].cpu().numpy().reshape(1, -1)\n",
    "        pred_original = scaler_y.inverse_transform(pred_scaled)[0]\n",
    "\n",
    "    return pred_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y_scaled, scaler_y = prepare_dataframe(df)\n",
    "\n",
    "# Convert targets to global tensor\n",
    "y_scaled_tensor = torch.tensor(y_scaled, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_data = build_heterodata(df)\n",
    "hetero_data = add_knn_edges(df, hetero_data, k=5)\n",
    "\n",
    "torch.save(hetero_data, \"hetero_data.pt\")\n",
    "print(\"hetero_data.pt saved with success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "hetero_data = hetero_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val, idx_test = create_splits(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_loaders(\n",
    "    hetero_data, idx_train, idx_val, idx_test, device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ENEMHeteroGNNOptimized(hetero_data, emb_dim=64, hidden_dim=128, out_dim=4)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "checkpoint_path = \"best_model.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_r2 = train_with_resume(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    scaler_y=scaler_y,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    max_epochs=1,\n",
    "    patience=20,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Melhor RÂ²:\", best_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate(model, test_loader, scaler_y, device=device)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_student(\n",
    "    model=model,\n",
    "    df_row=df.iloc[0],\n",
    "    hetero_data=hetero_data,\n",
    "    scaler_y=scaler_y\n",
    ")\n",
    "\n",
    "print(\"Prediction of the 4 tests:\", pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENEMHeteroGNNOptimized(nn.Module):\n",
    "    def __init__(self, data: HeteroData, emb_dim=64, hidden_dim=128, out_dim=4, dropout=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            nt: nn.Embedding(data[nt].num_nodes, emb_dim)\n",
    "            for nt in data.node_types\n",
    "        })\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_dim = emb_dim\n",
    "\n",
    "        for _ in range(3):\n",
    "            conv_dict = {et: SAGEConv((in_dim, in_dim), hidden_dim) for et in data.edge_types}\n",
    "            self.convs.append(HeteroConv(conv_dict, aggr=\"sum\"))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        self.norm_student = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data: HeteroData):\n",
    "        device = next(self.parameters()).device\n",
    "        x_dict = {}\n",
    "\n",
    "        for nt in self.embeddings:\n",
    "            if nt in data.node_types and hasattr(data[nt], \"n_id\"):\n",
    "                node_ids = data[nt].n_id.to(device)\n",
    "            else:\n",
    "                node_ids = torch.arange(\n",
    "                    self.embeddings[nt].num_embeddings,\n",
    "                    device=device\n",
    "                )\n",
    "            x_dict[nt] = self.embeddings[nt](node_ids)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, data.edge_index_dict)\n",
    "            x_dict = {nt: self.dropout(F.relu(v)) for nt, v in x_dict.items()}\n",
    "\n",
    "        s = x_dict[\"student\"]\n",
    "        s = self.norm_student(s)\n",
    "        out = self.head(s)\n",
    "\n",
    "        return {\"student\": out}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph\n",
    "hetero_data = torch.load(\"hetero_data.pt\", map_location=device)\n",
    "print(\"Graph loaded.\")\n",
    "print(\"Node types:\", hetero_data.node_types)\n",
    "print(\"Edge types:\", hetero_data.edge_types)\n",
    "\n",
    "# load checkpoint\n",
    "ckpt = torch.load(\"best_model.pt\", map_location=device)\n",
    "\n",
    "# build model exaclty as the train\n",
    "model = ENEMHeteroGNNOptimized(\n",
    "    hetero_data,\n",
    "    emb_dim=64,\n",
    "    hidden_dim=128,\n",
    "    out_dim=4,\n",
    "    dropout=0.4\n",
    ").to(device)\n",
    "\n",
    "# load weights\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "# load scaler\n",
    "scaler_y = ckpt[\"scaler_y\"]\n",
    "\n",
    "print(\"\\nModel uploaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"ENEM HeteroGNN Predictor\")\n",
    "\n",
    "class StudentRequest(BaseModel):\n",
    "    student_id: int  # student index in the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/predict\")\n",
    "def predict(request: StudentRequest):\n",
    "\n",
    "    student_id = request.student_id\n",
    "\n",
    "    # verify limits\n",
    "    if student_id >= hetero_data[\"student\"].num_nodes:\n",
    "        return {\"error\": \"Student ID out of bounds.\"}\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        out_dict = model(hetero_data)\n",
    "        preds = out_dict[\"student\"][student_id].cpu().numpy()\n",
    "\n",
    "    # desnormalize\n",
    "    preds = scaler_y.inverse_transform(preds.reshape(1, -1))[0]\n",
    "\n",
    "    return {\n",
    "        \"student_id\": student_id,\n",
    "        \"predicted_scores\": {\n",
    "            \"CN\": float(preds[0]),\n",
    "            \"CH\": float(preds[1]),\n",
    "            \"LC\": float(preds[2]),\n",
    "            \"MT\": float(preds[3])\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "config = Config(\n",
    "    app=app,\n",
    "    host=\"0.0.0.0\",\n",
    "    port=8000,\n",
    "    reload=False\n",
    ")\n",
    "\n",
    "server = Server(config)\n",
    "await server.serve()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
